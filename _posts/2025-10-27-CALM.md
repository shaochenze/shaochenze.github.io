---
layout: distill
title: Continuous Autoregressive Language Models
description: from discrete next-token prediction to continuous next-vector prediction
tags: Language Modeling
giscus_comments: true
date: 2025-11-01
featured: true
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

toc:
  - name: Introduction
  - name: Autoencoder
  - name: Likelihood-Free Language Modeling
  - name: Likelihood-Free LM Evaluation
  - name: Likelihood-Free Temperature Sampling
  - name: Performance
  - name: Concluding remarks
---

## Introduction

Large Language Models (LLMs) represent the central paradox of modern AI. On one hand, their capabilities are unprecedented. We've engineered models with hundreds of billions of parameters that can synthesize vast knowledge, execute intricate reasoning, and generate everything from nuanced prose to production-ready code. In short, we’ve built Ferrari-class engines.

And yet, we've placed them on a narrow country road, never letting it get out of first gear. This road is the dominant paradigm of autoregressive generation: predicting text one discrete token at a time. No matter how powerful the engine, its throughput is ultimately bottlenecked by the road. This mismatch is why state-of-the-art LLMs are so inefficient and computationally expensive to run.

Historically, we have tried to widen this road by making text units larger—moving from individual characters to subword tokens. This helped, but we've hit the physical limits of that approach. With a typical 32K vocabulary, the semantic bandwidth of each generative step is approximately 15 bits (log_2 32768 = 15). To double this bandwidth, the required vocabulary size would have to grow exponentially to 2^30 entries, making the model computationally infeasible. This is the scaling wall of discrete tokens.

If the discrete, token-by-token approach is the narrow country road, we need to build a new kind of highway. This is the core idea of Continuous Autoregressive Language Models (CALM): we shift from the discrete domain to a continuous one. Instead of predicting the next token, CALM predicts the next vector—a single, dense vector that represents a K-token chunk of text.

<div class='l-body'>
<img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/CALM/calm_fig1.png">
</div>

In this post, we'll focus on the core ideas that make CALM work and omit some of the architectural details. We will cover the key challenges of modeling in a continuous space, the likelihood-free toolkit we built to solve them, and the experimental benefits of adding this new scaling axis of semantic bandwidth.

## Autoencoder

## Likelihood-Free Language Modeling

## Likelihood-Free LM Evaluation

## Likelihood-Free Temperature Sampling

## Performance

## Concluding remarks
