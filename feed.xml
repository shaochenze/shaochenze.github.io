<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://shaochenze.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shaochenze.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-31T08:07:16+00:00</updated><id>https://shaochenze.github.io/feed.xml</id><title type="html">Chenze Shao</title><subtitle>The personal website of Chenze Shao. </subtitle><entry><title type="html">Continuous Autoregressive Language Models</title><link href="https://shaochenze.github.io/blog/2025/CALM/" rel="alternate" type="text/html" title="Continuous Autoregressive Language Models"/><published>2025-10-30T00:00:00+00:00</published><updated>2025-10-30T00:00:00+00:00</updated><id>https://shaochenze.github.io/blog/2025/CALM</id><content type="html" xml:base="https://shaochenze.github.io/blog/2025/CALM/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Large Language Models (LLMs) represent the central paradox of modern AI. On one hand, their capabilities are unprecedented. We’ve engineered models with hundreds of billions of parameters that can synthesize vast knowledge, execute intricate reasoning, and generate everything from nuanced prose to production-ready code. In short, we’ve built Ferrari-class engines.</p> <p>And yet, we’ve placed them on a narrow country road, never letting it get out of first gear. This road is the dominant paradigm of autoregressive generation: predicting text one discrete token at a time. No matter how powerful the engine, its throughput is ultimately bottlenecked by the road. This mismatch is why state-of-the-art LLMs are so inefficient and computationally expensive to run.</p> <p>Historically, we have tried to widen this road by making text units larger—moving from individual characters to subword tokens. This helped, but we’ve hit the physical limits of that approach. With a typical 32K vocabulary, the semantic bandwidth of each generative step is approximately 15 bits ($\log_2 32768 = 15$). To double this bandwidth, the required vocabulary size would have to grow exponentially to $2^{30}$ entries, making the model computationally infeasible. This is the scaling wall of discrete tokens.</p> <p>If the discrete, token-by-token approach is the narrow country road, we need to build a new kind of highway. This is the core idea of Continuous Autoregressive Language Models (CALM): we shift from the discrete domain to a continuous one. Instead of predicting the next token, CALM predicts the next vector—a single, dense vector that represents a K-token chunk of text.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/img/CALM/calm_fig1.png"/> </div> <p>In this post, we’ll focus on the core ideas that make CALM work and omit some of the architectural details. We will cover the key challenges of modeling in a continuous space, the likelihood-free toolkit we built to solve them, and the experimental benefits of adding this new scaling axis of semantic bandwidth.</p> <h2 id="autoencoder">Autoencoder</h2> <p>The first step in building CALM is to train a high-fidelity autoencoder to establish a <strong>bidirectional mapping</strong> between discrete tokens and continuous vectors. The autoencoder is composed of two parts:</p> <ul> <li>Encoder \(f_{enc}: \mathcal{V}^K \to \mathbb{R}^l\), which takes a chunk of K tokens and compress them into a single continuous vector.</li> <li>Decoder \(g_{dec}: \mathbb{R}^l \to \mathcal{V}^K\), which takes that vector and reconstructs the original tokens.</li> </ul> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/CALM/calm_fig2.png"/> </div> <p>We won’t delve into the specific architecture of the encoder and decoder in this post. They are simply composed of stacked MLP layers. The autoencoder is trained to ensure that the output of $g_{dec}(f_{enc}(\mathbf{x}<em>{1:K}))$ closely approximates the input $\mathbf{x}</em>{1:K}$. This is achieved by minimizing the cross-entropy loss:</p> \[\mathcal{L}_{\text{ae}}(\mathbf{x}_{1:K}) = - \sum_{i=1}^{K} \log p_{dec}(x_i | \mathbf{z}=f_{\text{enc}}(\mathbf{x}_{1:K})).\] <p>For this framework to succeed, the reconstruction must be near-perfect. This is theoretically feasible when we compare the information capacity of the two representations. A discrete token contains only about 10-20 bits of information, whereas a floating-point continuous vector can store $32l$ bits. Therefore, a single vector can theoretically encapsulate the information of $K\approx 2l$ tokens through simple hard-coding. Practically, when compressing a chunk of \(K=4\) tokens into a single vector, we found that a latent vector of \(l=10\) is sufficient to achieve high-fidelity reconstruction, with a token-level accuracy of over <strong>99.9%</strong>.</p> <p>However, high reconstruction accuracy alone is insufficient for a robust generative framework. An autoencoder optimized solely for reconstruction learns an exceptionally brittle representation. In such a space, a minor perturbation to the vector can cause the decoder to reconstruct completely unrelated tokens. To solve this, we need a smoother mapping to tolerate a certain amount of noise. Our primary strategy is to smooth the latent manifold by moving from a deterministic autoencoder to a <strong>variational one</strong>, where the encoder learns to represent tokens as a Gaussian distribution. Additionally, we incorporate dropout techniques, which force the autoencoder to learn a redundant vector representation.</p> <p>The synthesis of these techniques results in a powerful and robust autoencoder. Our final model maps a chunk of \(K=4\) tokens into a \(l=128\) dimensional vector. It can withstand substantial Gaussian noise of $\sigma \approx 0.3$ and still reconstruct the original tokens with over <strong>99.9%</strong> accuracy. This lays a foundation for the subsequent learning of CALM.</p> <h2 id="likelihood-free-language-modeling">Likelihood-Free Language Modeling</h2> <h2 id="likelihood-free-lm-evaluation">Likelihood-Free LM Evaluation</h2> <h2 id="likelihood-free-temperature-sampling">Likelihood-Free Temperature Sampling</h2> <h2 id="performance">Performance</h2> <h2 id="concluding-remarks">Concluding remarks</h2>]]></content><author><name></name></author><category term="Language"/><category term="Modeling"/><summary type="html"><![CDATA[from discrete next-token prediction to continuous next-vector prediction]]></summary></entry></feed>